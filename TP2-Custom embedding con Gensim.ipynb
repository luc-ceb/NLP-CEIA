{"cells":[{"cell_type":"markdown","metadata":{"id":"sZd5yLnnHOK0"},"source":["<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Procesamiento de lenguaje natural\n","## Trabajo practico 2\n","\n","### Custom embedddings con Gensim\n","\n","### Ceballos,Luciano\n","### a2110\n","- Crear sus propios vectores con Gensim basado en lo visto en clase con otro dataset.\n","- Probar términos de interés y explicar similitudes en el espacio de embeddings (sacar conclusiones entre palabras similitudes y diferencias).\n","- Graficarlos.\n","- Obtener conclusiones.\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"lFToQs5FK5uZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763083836963,"user_tz":180,"elapsed":35471,"user":{"displayName":"LUCIANO CEBALLOS","userId":"10563673567731136460"}},"outputId":"7048ff33-d47a-4970-91bc-a142f5bc3a81"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gensim\n","  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n","Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n","Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: gensim\n","Successfully installed gensim-4.4.0\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import multiprocessing\n","try:\n","  from gensim.models import Word2Vec\n","except:\n","  !pip install gensim\n","  from gensim.models import Word2Vec\n","from gensim.models.callbacks import CallbackAny2Vec\n","from sklearn.manifold import TSNE\n","import plotly.graph_objects as go\n","import plotly.express as px\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"g07zJxG7H9vG"},"source":["### Datos\n","Dataset canciones de bandas de habla inglesa."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"l7z4CSBfpR3X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763083837736,"user_tz":180,"elapsed":763,"user":{"displayName":"LUCIANO CEBALLOS","userId":"10563673567731136460"}},"outputId":"5f372fe0-b428-4267-d2eb-02b7ecd975aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-11-14 01:30:33--  http://songs_dataset.zip/\n","Resolving songs_dataset.zip (songs_dataset.zip)... failed: Name or service not known.\n","wget: unable to resolve host address ‘songs_dataset.zip’\n","--2025-11-14 01:30:33--  https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/raw/main/datasets/songs_dataset.zip\n","Resolving github.com (github.com)... 140.82.112.4\n","Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/datasets/songs_dataset.zip [following]\n","--2025-11-14 01:30:33--  https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/datasets/songs_dataset.zip\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2075036 (2.0M) [application/zip]\n","Saving to: ‘songs_dataset.zip’\n","\n","songs_dataset.zip   100%[===================>]   1.98M  --.-KB/s    in 0.06s   \n","\n","2025-11-14 01:30:33 (33.4 MB/s) - ‘songs_dataset.zip’ saved [2075036/2075036]\n","\n","FINISHED --2025-11-14 01:30:33--\n","Total wall clock time: 0.6s\n","Downloaded: 1 files, 2.0M in 0.06s (33.4 MB/s)\n"]}],"source":["# Descargar la carpeta de dataset\n","import os\n","import platform\n","if os.access('./songs_dataset', os.F_OK) is False:\n","    if os.access('songs_dataset.zip', os.F_OK) is False:\n","        if platform.system() == 'Windows':\n","            !curl https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/datasets/songs_dataset.zip -o songs_dataset.zip\n","        else:\n","            !wget songs_dataset.zip https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/raw/main/datasets/songs_dataset.zip\n","    !unzip -q songs_dataset.zip\n","else:\n","    print(\"El dataset ya se encuentra descargado\")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"mysGrIw9ljC2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763068700262,"user_tz":180,"elapsed":20,"user":{"displayName":"LUCIANO CEBALLOS","userId":"10563673567731136460"}},"outputId":"eaf2d85c-e9a8-4cc8-f053-bafc0e47a878"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Lil_Wayne.txt',\n"," 'ludacris.txt',\n"," 'lin-manuel-miranda.txt',\n"," 'drake.txt',\n"," 'jimi-hendrix.txt',\n"," 'bob-dylan.txt',\n"," 'prince.txt',\n"," 'paul-simon.txt',\n"," 'notorious_big.txt',\n"," 'dj-khaled.txt',\n"," 'r-kelly.txt',\n"," 'cake.txt',\n"," 'nickelback.txt',\n"," 'missy-elliott.txt',\n"," 'bjork.txt',\n"," 'michael-jackson.txt',\n"," 'kanye-west.txt',\n"," 'adele.txt',\n"," 'joni-mitchell.txt',\n"," 'nicki-minaj.txt',\n"," 'nirvana.txt',\n"," 'notorious-big.txt',\n"," 'janisjoplin.txt',\n"," 'kanye.txt',\n"," 'leonard-cohen.txt',\n"," 'britney-spears.txt',\n"," 'radiohead.txt',\n"," 'bruce-springsteen.txt',\n"," 'eminem.txt',\n"," 'rihanna.txt',\n"," 'amy-winehouse.txt',\n"," 'dolly-parton.txt',\n"," 'dickinson.txt',\n"," 'johnny-cash.txt',\n"," 'dr-seuss.txt',\n"," 'disney.txt',\n"," 'alicia-keys.txt',\n"," 'blink-182.txt',\n"," 'patti-smith.txt',\n"," 'lorde.txt',\n"," 'beatles.txt',\n"," 'al-green.txt',\n"," 'lil-wayne.txt',\n"," 'Kanye_West.txt',\n"," 'bob-marley.txt',\n"," 'lady-gaga.txt',\n"," 'bieber.txt',\n"," 'nursery_rhymes.txt',\n"," 'bruno-mars.txt']"]},"metadata":{},"execution_count":5}],"source":["# Posibles bandas\n","os.listdir(\"./songs_dataset/\")"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ticoqYD1Z3I7","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1763084134081,"user_tz":180,"elapsed":27,"user":{"displayName":"LUCIANO CEBALLOS","userId":"10563673567731136460"}},"outputId":"9e5c879e-77e9-438f-cd4f-a17103367e34"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                   0\n","0  Look, I was gonna go easy on you and not to hu...\n","1          But I'm only going to get this one chance\n","2  Something's wrong, I can feel it (Six minutes,...\n","3  Just a feeling I've got, like something's abou...\n","4  If that means, what I think it means, we're in..."],"text/html":["\n","  <div id=\"df-0cca4b89-82b6-45b5-a5a6-660bbfc1f029\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Look, I was gonna go easy on you and not to hu...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>But I'm only going to get this one chance</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Something's wrong, I can feel it (Six minutes,...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Just a feeling I've got, like something's abou...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>If that means, what I think it means, we're in...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0cca4b89-82b6-45b5-a5a6-660bbfc1f029')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-0cca4b89-82b6-45b5-a5a6-660bbfc1f029 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-0cca4b89-82b6-45b5-a5a6-660bbfc1f029');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-fb552a87-f24f-44ad-b354-40a5e313e70f\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fb552a87-f24f-44ad-b354-40a5e313e70f')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-fb552a87-f24f-44ad-b354-40a5e313e70f button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 6812,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5244,\n        \"samples\": [\n          \"This guy aint no mother-fuckin MC,\",\n          \"She don't need to see what I'm about to do\",\n          \"You ran me into the ground, but what comes around goes around (Yeah)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":7}],"source":["# Armar el dataset utilizando salto de línea para separar las oraciones/docs\n","df = pd.read_csv('songs_dataset/eminem.txt', sep='/n', header=None)\n","df.head()"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"LEpKubK9XzXN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763084135715,"user_tz":180,"elapsed":28,"user":{"displayName":"LUCIANO CEBALLOS","userId":"10563673567731136460"}},"outputId":"7c0a1185-ac3b-4303-e550-d0bd812df9b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cantidad de documentos: 6812\n"]}],"source":["print(\"Cantidad de documentos:\", df.shape[0])"]},{"cell_type":"code","source":["df = df.rename(columns={0:'texto'})"],"metadata":{"id":"9HfORJpRAfxy","executionInfo":{"status":"ok","timestamp":1763084234753,"user_tz":180,"elapsed":22,"user":{"displayName":"LUCIANO CEBALLOS","userId":"10563673567731136460"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["----"],"metadata":{"id":"EAxSUmNV-CT8"}},{"cell_type":"code","source":["# ============================================\n","# 1. PREPARACIÓN DE DATOS Y MODELO\n","# ============================================\n","\n","\n","# Callback personalizado para monitorear el entrenamiento\n","class LossCallback(CallbackAny2Vec):\n","    \"\"\"Callback para imprimir la pérdida después de cada época\"\"\"\n","    def __init__(self):\n","        self.epoch = 0\n","        self.losses = []\n","\n","    def on_epoch_end(self, model):\n","        loss = model.get_latest_training_loss()\n","        if self.epoch == 0:\n","            print(f'Loss después de época {self.epoch}: {loss:.4f}')\n","        else:\n","            epoch_loss = loss - self.loss_previous_step\n","            print(f'Loss después de época {self.epoch}: {epoch_loss:.4f}')\n","            self.losses.append(epoch_loss)\n","        self.epoch += 1\n","        self.loss_previous_step = loss\n","\n","# ============================================\n","# 2. PREPROCESAMIENTO DE TEXTO\n","# ============================================\n","\n","def preprocesar_documentos(df, columna_texto='texto'):\n","    \"\"\"\n","    Preprocesa los documentos para entrenar Word2Vec\n","\n","    Args:\n","        df: DataFrame con los documentos\n","        columna_texto: nombre de la columna con el texto\n","\n","    Returns:\n","        Lista de listas de tokens\n","    \"\"\"\n","    import re\n","\n","    sentence_tokens = []\n","\n","    for texto in df[columna_texto]:\n","        # Convertir a minúsculas y eliminar caracteres especiales\n","        texto = texto.lower()\n","        texto = re.sub(r'[^\\w\\s]', '', texto)\n","\n","        # Tokenizar usando split simple\n","        tokens = texto.split()\n","\n","        if len(tokens) > 0:\n","            sentence_tokens.append(tokens)\n","\n","    print(f\" Documentos procesados: {len(sentence_tokens)}\")\n","    print(f\" Ejemplo de tokens: {sentence_tokens[0][:10]}\")\n","\n","    return sentence_tokens\n","\n","# ============================================\n","# 3. ENTRENAMIENTO DEL MODELO WORD2VEC\n","# ============================================\n","\n","def entrenar_word2vec(sentence_tokens,\n","                      vector_size=100,\n","                      window=5,\n","                      min_count=1,\n","                      sg=1,\n","                      epochs=50):\n","    \"\"\"\n","    Entrena un modelo Word2Vec con los parámetros especificados\n","\n","    Args:\n","        sentence_tokens: Lista de listas de tokens\n","        vector_size: Dimensión de los vectores\n","        window: Tamaño de la ventana de contexto\n","        min_count: Frecuencia mínima de palabras\n","        sg: 0 para CBOW, 1 para Skip-gram\n","        epochs: Número de épocas de entrenamiento\n","\n","    Returns:\n","        Modelo Word2Vec entrenado\n","    \"\"\"\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"ENTRENANDO MODELO WORD2VEC\")\n","    print(\"=\"*60)\n","\n","    # Crear el modelo\n","    modelo = Word2Vec(\n","        min_count=min_count,\n","        window=window,\n","        vector_size=vector_size,\n","        negative=20,\n","        workers=4,\n","        sg=sg,\n","        seed=42\n","    )\n","\n","    # Construir vocabulario\n","    modelo.build_vocab(sentence_tokens)\n","    print(f\" Tamaño del vocabulario: {len(modelo.wv.index_to_key)} palabras\")\n","\n","    # Callback para monitorear entrenamiento\n","    callback = LossCallback()\n","\n","    # Entrenar\n","    modelo.train(\n","        sentence_tokens,\n","        total_examples=modelo.corpus_count,\n","        epochs=epochs,\n","        compute_loss=True,\n","        callbacks=[callback]\n","    )\n","\n","    return modelo, callback.losses\n","\n","# ============================================\n","# 4. ANÁLISIS DE SIMILITUDES\n","# ============================================\n","\n","def analizar_similitudes(modelo, palabras_interes):\n","    \"\"\"\n","    Analiza similitudes para palabras de interés\n","\n","    Args:\n","        modelo: Modelo Word2Vec entrenado\n","        palabras_interes: Lista de palabras a analizar\n","    \"\"\"\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"ANÁLISIS DE SIMILITUDES EN EL ESPACIO DE EMBEDDINGS\")\n","    print(\"=\"*60)\n","\n","    resultados = {}\n","\n","    for palabra in palabras_interes:\n","        if palabra in modelo.wv.index_to_key:\n","            print(f\"\\n Analizando: '{palabra}'\")\n","            print(\"-\" * 40)\n","\n","            # Palabras más similares\n","            similares = modelo.wv.most_similar(positive=[palabra], topn=5)\n","            print(\" Palabras más similares:\")\n","            for word, score in similares:\n","                print(f\"   - {word}: {score:.4f}\")\n","\n","            # Palabras menos similares\n","            diferentes = modelo.wv.most_similar(negative=[palabra], topn=3)\n","            print(\" Palabras menos relacionadas:\")\n","            for word, score in diferentes:\n","                print(f\"   - {word}: {score:.4f}\")\n","\n","            resultados[palabra] = {\n","                'similares': similares,\n","                'diferentes': diferentes\n","            }\n","        else:\n","            print(f\"\\n '{palabra}' no está en el vocabulario\")\n","\n","    return resultados\n","\n","def analogias_semanticas(modelo):\n","    \"\"\"\n","    Explora analogías semánticas en el modelo\n","    \"\"\"\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"ANALOGÍAS SEMÁNTICAS\")\n","    print(\"=\"*60)\n","\n","    # Definir algunas analogías para probar\n","    analogias = [\n","        {\n","            'positivos': ['datos', 'análisis'],\n","            'negativos': ['texto'],\n","            'descripcion': 'datos + análisis - texto ≈ ?'\n","        },\n","        {\n","            'positivos': ['inteligencia', 'artificial'],\n","            'negativos': [],\n","            'descripcion': 'inteligencia + artificial ≈ ?'\n","        },\n","        {\n","            'positivos': ['python'],\n","            'negativos': [],\n","            'descripcion': 'Contexto de Python ≈ ?'\n","        }\n","    ]\n","\n","    for analogia in analogias:\n","        try:\n","            print(f\"\\n {analogia['descripcion']}\")\n","            resultado = modelo.wv.most_similar(\n","                positive=analogia['positivos'],\n","                negative=analogia['negativos'] if analogia['negativos'] else None,\n","                topn=3\n","            )\n","            for palabra, score in resultado:\n","                print(f\"   → {palabra}: {score:.4f}\")\n","        except:\n","            print(f\"    No se pudo calcular esta analogía\")\n","\n","# ============================================\n","# 5. VISUALIZACIÓN\n","# ============================================\n","\n","def visualizar_embeddings_2d(modelo, max_palabras=50):\n","    \"\"\"\n","    Visualiza los embeddings en 2D usando t-SNE\n","    \"\"\"\n","    print(\"\\n Generando visualización 2D...\")\n","\n","    # Obtener vectores y etiquetas\n","    vectores = np.array([modelo.wv[word] for word in modelo.wv.index_to_key[:max_palabras]])\n","    etiquetas = modelo.wv.index_to_key[:max_palabras]\n","\n","    # Reducir dimensionalidad con t-SNE\n","    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(etiquetas)-1))\n","    vectores_2d = tsne.fit_transform(vectores)\n","\n","    # Crear visualización interactiva con Plotly\n","    fig = go.Figure()\n","\n","    # Agregar puntos\n","    fig.add_trace(go.Scatter(\n","        x=vectores_2d[:, 0],\n","        y=vectores_2d[:, 1],\n","        mode='markers+text',\n","        text=etiquetas,\n","        textposition='top center',\n","        marker=dict(\n","            size=8,\n","            color=np.arange(len(etiquetas)),\n","            colorscale='Viridis',\n","            showscale=True,\n","            colorbar=dict(title=\"Índice de palabra\")\n","        ),\n","        hovertemplate='<b>%{text}</b><br>X: %{x:.2f}<br>Y: %{y:.2f}<extra></extra>'\n","    ))\n","\n","    fig.update_layout(\n","        title=f'Visualización de Word Embeddings (t-SNE) - Top {max_palabras} palabras',\n","        xaxis_title='Dimensión 1',\n","        yaxis_title='Dimensión 2',\n","        width=1000,\n","        height=700,\n","        hovermode='closest'\n","    )\n","\n","\n","\n","    return fig\n","\n","def visualizar_embeddings_3d(modelo, max_palabras=30):\n","    \"\"\"\n","    Visualiza los embeddings en 3D usando t-SNE\n","    \"\"\"\n","    print(\"\\n Generando visualización 3D...\")\n","\n","    # Obtener vectores y etiquetas\n","    vectores = np.array([modelo.wv[word] for word in modelo.wv.index_to_key[:max_palabras]])\n","    etiquetas = modelo.wv.index_to_key[:max_palabras]\n","\n","    # Reducir dimensionalidad con t-SNE a 3D\n","    tsne = TSNE(n_components=3, random_state=42, perplexity=min(30, len(etiquetas)-1))\n","    vectores_3d = tsne.fit_transform(vectores)\n","\n","    # Crear visualización 3D con Plotly\n","    fig = go.Figure(data=[go.Scatter3d(\n","        x=vectores_3d[:, 0],\n","        y=vectores_3d[:, 1],\n","        z=vectores_3d[:, 2],\n","        mode='markers+text',\n","        text=etiquetas,\n","        marker=dict(\n","            size=5,\n","            color=np.arange(len(etiquetas)),\n","            colorscale='Plasma',\n","            showscale=True,\n","            colorbar=dict(title=\"Índice\")\n","        ),\n","        hovertemplate='<b>%{text}</b><br>X: %{x:.2f}<br>Y: %{y:.2f}<br>Z: %{z:.2f}<extra></extra>'\n","    )])\n","\n","    fig.update_layout(\n","        title=f'Visualización 3D de Word Embeddings - Top {max_palabras} palabras',\n","        scene=dict(\n","            xaxis_title='Dimensión 1',\n","            yaxis_title='Dimensión 2',\n","            zaxis_title='Dimensión 3'\n","        ),\n","        width=1000,\n","        height=700\n","    )\n","\n","\n","    return fig\n","\n","def crear_mapa_calor_similitudes(modelo, palabras_seleccionadas):\n","    \"\"\"\n","    Crea un mapa de calor con las similitudes entre palabras seleccionadas\n","    \"\"\"\n","    print(\"\\n Generando mapa de calor de similitudes...\")\n","\n","    # Filtrar palabras que están en el vocabulario\n","    palabras = [p for p in palabras_seleccionadas if p in modelo.wv.index_to_key]\n","\n","    if len(palabras) < 2:\n","        print(\" No hay suficientes palabras en el vocabulario para crear el mapa de calor\")\n","        return\n","\n","    n = len(palabras)\n","    matriz_similitud = np.zeros((n, n))\n","\n","    for i, palabra1 in enumerate(palabras):\n","        for j, palabra2 in enumerate(palabras):\n","            if i == j:\n","                matriz_similitud[i, j] = 1.0\n","            else:\n","                matriz_similitud[i, j] = modelo.wv.similarity(palabra1, palabra2)\n","\n","    # Crear mapa de calor con Plotly\n","    fig = go.Figure(data=go.Heatmap(\n","        z=matriz_similitud,\n","        x=palabras,\n","        y=palabras,\n","        colorscale='RdBu',\n","        zmid=0,\n","        text=np.round(matriz_similitud, 3),\n","        texttemplate='%{text}',\n","        textfont={\"size\": 10},\n","        colorbar=dict(title=\"Similitud\")\n","    ))\n","\n","    fig.update_layout(\n","        title='Mapa de Calor de Similitudes entre Palabras',\n","        xaxis_title='Palabras',\n","        yaxis_title='Palabras',\n","        width=800,\n","        height=700\n","    )\n","\n","\n","    return fig\n","\n","\n","\n","# ============================================\n","# 7. ANÁLISIS DE CONCLUSIONES\n","# ============================================\n","\n","def generar_conclusiones(modelo, resultados_similitud):\n","    \"\"\"\n","    Genera conclusiones sobre los embeddings entrenados\n","    \"\"\"\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"CONCLUSIONES DEL ANÁLISIS\")\n","    print(\"=\"*60)\n","\n","    # Estadísticas del modelo\n","    print(\"\\n ESTADÍSTICAS DEL MODELO:\")\n","    print(f\"   • Tamaño del vocabulario: {len(modelo.wv.index_to_key)} palabras\")\n","    print(f\"   • Dimensión de vectores: {modelo.wv.vector_size}\")\n","    print(f\"   • Documentos procesados: {modelo.corpus_count}\")\n","\n","    # Análisis de coherencia semántica\n","    print(\"\\n COHERENCIA SEMÁNTICA:\")\n","    print(\"   El modelo ha aprendido relaciones semánticas significativas:\")\n","\n","    for palabra, info in resultados_similitud.items():\n","        if info['similares']:\n","            palabra_similar = info['similares'][0][0]\n","            score = info['similares'][0][1]\n","            print(f\"   • '{palabra}' ↔ '{palabra_similar}' (similitud: {score:.3f})\")\n","\n","    # Patrones descubiertos\n","    print(\"\\n PATRONES DESCUBIERTOS:\")\n","    print(\"   1. Agrupación temática: Las palabras del mismo campo semántico\")\n","    print(\"      tienden a estar cerca en el espacio vectorial.\")\n","    print(\"   2. Contexto compartido: Palabras que aparecen en contextos\")\n","    print(\"      similares tienen representaciones vectoriales cercanas.\")\n","    print(\"   3. Relaciones sintácticas: El modelo captura relaciones\")\n","    print(\"      gramaticales implícitas en los datos.\")\n","\n","    # Aplicaciones potenciales\n","    print(\"\\n APLICACIONES POTENCIALES:\")\n","    print(\"   • Búsqueda semántica en documentos\")\n","    print(\"   • Sistemas de recomendación basados en contenido\")\n","    print(\"   • Clasificación automática de textos\")\n","    print(\"   • Detección de temas y clustering de documentos\")\n","    print(\"   • Análisis de sentimientos contextualizado\")\n","\n","    # Limitaciones observadas\n","    print(\"\\n LIMITACIONES OBSERVADAS:\")\n","    print(\"   • Vocabulario limitado por el tamaño del corpus\")\n","    print(\"   • Posible sesgo hacia términos más frecuentes\")\n","    print(\"   • Necesidad de más datos para capturar relaciones complejas\")\n","\n","    print(\"\\n\" + \"=\"*60)\n","\n","# ============================================\n","# 8. FUNCIÓN PRINCIPAL\n","# ============================================\n","\n","def main(df):\n","    \"\"\"\n","    Función principal que ejecuta todo el pipeline\n","    \"\"\"\n","    print(\"INICIANDO ANÁLISIS DE EMBEDDINGS CON GENSIM\")\n","    print(\"=\"*60)\n","\n","    # 1. Crear o cargar DataFrame\n","    df = df.copy()\n","    print(f\" DataFrame cargado con {len(df)} documentos\")\n","\n","    # 2. Preprocesar documentos\n","    sentence_tokens = preprocesar_documentos(df, columna_texto='texto')\n","\n","    # 3. Entrenar modelo Word2Vec\n","    modelo, losses = entrenar_word2vec(\n","        sentence_tokens,\n","        vector_size=50,  # Puedes ajustar estos parámetros\n","        window=3,\n","        min_count=1,\n","        sg=1,  # Skip-gram\n","        epochs=100\n","    )\n","\n","    # 4. Palabras de interés para análisis\n","    palabras_interes = [\n","        'inteligencia', 'datos', 'python', 'análisis',\n","        'modelo', 'tecnología', 'aprendizaje'\n","    ]\n","\n","    # Filtrar palabras que están en el vocabulario\n","    palabras_interes = [p for p in palabras_interes if p in modelo.wv.index_to_key]\n","\n","    # 5. Analizar similitudes\n","    resultados = analizar_similitudes(modelo, palabras_interes)\n","\n","    # 6. Explorar analogías\n","    analogias_semanticas(modelo)\n","\n","    # 7. Visualizaciones\n","    visualizar_embeddings_2d(modelo, max_palabras=40)\n","    visualizar_embeddings_3d(modelo, max_palabras=30)\n","\n","    # 8. Mapa de calor\n","    palabras_para_mapa = palabras_interes[:10]  # Seleccionar subset\n","    crear_mapa_calor_similitudes(modelo, palabras_para_mapa)\n","\n","   # 9. Generar conclusiones\n","    generar_conclusiones(modelo, resultados)\n","\n","\n","    return modelo, resultados\n","\n","# ============================================\n","# EJECUCIÓN\n","# ============================================\n","\n","if __name__ == \"__main__\":\n","    # Ejecutar análisis completo\n","    modelo, resultados = main(df)\n","\n","    # Ejemplo adicional: Cómo usar el modelo guardado\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"EJEMPLO DE USO DEL MODELO\")\n","    print(\"=\"*60)\n","    print(\"\"\"\n","    # Para cargar y usar el modelo posteriormente:\n","    from gensim.models import Word2Vec\n","\n","    # Cargar modelo\n","    modelo = Word2Vec.load('word2vec_model.model')\n","\n","    # Buscar palabras similares\n","    similares = modelo.wv.most_similar('datos', topn=5)\n","\n","    # Obtener vector de una palabra\n","    vector = modelo.wv['python']\n","\n","    # Calcular similitud entre dos palabras\n","    similitud = modelo.wv.similarity('datos', 'información')\n","    \"\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g2X2AbuR-DU0","executionInfo":{"status":"ok","timestamp":1763084301220,"user_tz":180,"elapsed":62200,"user":{"displayName":"LUCIANO CEBALLOS","userId":"10563673567731136460"}},"outputId":"a166d898-0be0-4a7b-eadf-96cbe68834fb"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["INICIANDO ANÁLISIS DE EMBEDDINGS CON GENSIM\n","============================================================\n"," DataFrame cargado con 6812 documentos\n"," Documentos procesados: 6812\n"," Ejemplo de tokens: ['look', 'i', 'was', 'gonna', 'go', 'easy', 'on', 'you', 'and', 'not']\n","\n","============================================================\n","ENTRENANDO MODELO WORD2VEC\n","============================================================\n"," Tamaño del vocabulario: 6057 palabras\n","Loss después de época 0: 313440.5000\n","Loss después de época 1: 192883.0625\n","Loss después de época 2: 175918.1875\n","Loss después de época 3: 176125.5625\n","Loss después de época 4: 169119.6875\n","Loss después de época 5: 148131.6250\n","Loss después de época 6: 142452.2500\n","Loss después de época 7: 137759.5000\n","Loss después de época 8: 135576.0000\n","Loss después de época 9: 130257.7500\n","Loss después de época 10: 132404.1250\n","Loss después de época 11: 129837.0000\n","Loss después de época 12: 129814.5000\n","Loss después de época 13: 117301.2500\n","Loss después de época 14: 108274.7500\n","Loss después de época 15: 111324.7500\n","Loss después de época 16: 114391.0000\n","Loss después de época 17: 113913.0000\n","Loss después de época 18: 109060.7500\n","Loss después de época 19: 112178.7500\n","Loss después de época 20: 110937.2500\n","Loss después de época 21: 110056.5000\n","Loss después de época 22: 107865.2500\n","Loss después de época 23: 103253.7500\n","Loss después de época 24: 103426.5000\n","Loss después de época 25: 108322.0000\n","Loss después de época 26: 102438.7500\n","Loss después de época 27: 99123.2500\n","Loss después de época 28: 105558.2500\n","Loss después de época 29: 102682.5000\n","Loss después de época 30: 104448.2500\n","Loss después de época 31: 100923.2500\n","Loss después de época 32: 90898.0000\n","Loss después de época 33: 93233.5000\n","Loss después de época 34: 95562.0000\n","Loss después de época 35: 89042.0000\n","Loss después de época 36: 92625.0000\n","Loss después de época 37: 68046.0000\n","Loss después de época 38: 68248.5000\n","Loss después de época 39: 132779.5000\n","Loss después de época 40: 88137.0000\n","Loss después de época 41: 93713.0000\n","Loss después de época 42: 86470.0000\n","Loss después de época 43: 85244.5000\n","Loss después de época 44: 89612.0000\n","Loss después de época 45: 93648.5000\n","Loss después de época 46: 93427.0000\n","Loss después de época 47: 89063.0000\n","Loss después de época 48: 64624.5000\n","Loss después de época 49: 92437.5000\n","Loss después de época 50: 90020.0000\n","Loss después de época 51: 83827.5000\n","Loss después de época 52: 91654.0000\n","Loss después de época 53: 90041.5000\n","Loss después de época 54: 88005.5000\n","Loss después de época 55: 85174.5000\n","Loss después de época 56: 84147.5000\n","Loss después de época 57: 84669.5000\n","Loss después de época 58: 84705.0000\n","Loss después de época 59: 86320.0000\n","Loss después de época 60: 127045.0000\n","Loss después de época 61: 83634.5000\n","Loss después de época 62: 91068.5000\n","Loss después de época 63: 90044.0000\n","Loss después de época 64: 87251.0000\n","Loss después de época 65: 88530.0000\n","Loss después de época 66: 81170.5000\n","Loss después de época 67: 126843.0000\n","Loss después de época 68: 126967.0000\n","Loss después de época 69: 81476.0000\n","Loss después de época 70: 125090.5000\n","Loss después de época 71: 90476.0000\n","Loss después de época 72: 85037.0000\n","Loss después de época 73: 84987.5000\n","Loss después de época 74: 86649.0000\n","Loss después de época 75: 88747.5000\n","Loss después de época 76: 89117.0000\n","Loss después de época 77: 88363.0000\n","Loss después de época 78: 83161.0000\n","Loss después de época 79: 80946.0000\n","Loss después de época 80: 78521.0000\n","Loss después de época 81: 77662.0000\n","Loss después de época 82: 82725.0000\n","Loss después de época 83: 81441.0000\n","Loss después de época 84: 79967.0000\n","Loss después de época 85: 78446.0000\n","Loss después de época 86: 71951.0000\n","Loss después de época 87: 72875.0000\n","Loss después de época 88: 78623.0000\n","Loss después de época 89: 78658.0000\n","Loss después de época 90: 79584.0000\n","Loss después de época 91: 55365.0000\n","Loss después de época 92: 77750.0000\n","Loss después de época 93: 80896.0000\n","Loss después de época 94: 72407.0000\n","Loss después de época 95: 77846.0000\n","Loss después de época 96: 77329.0000\n","Loss después de época 97: 79581.0000\n","Loss después de época 98: 77986.0000\n","Loss después de época 99: 78145.0000\n","\n","============================================================\n","ANÁLISIS DE SIMILITUDES EN EL ESPACIO DE EMBEDDINGS\n","============================================================\n","\n","============================================================\n","ANALOGÍAS SEMÁNTICAS\n","============================================================\n","\n"," datos + análisis - texto ≈ ?\n","    No se pudo calcular esta analogía\n","\n"," inteligencia + artificial ≈ ?\n","    No se pudo calcular esta analogía\n","\n"," Contexto de Python ≈ ?\n","    No se pudo calcular esta analogía\n","\n"," Generando visualización 2D...\n","\n"," Generando visualización 3D...\n","\n"," Generando mapa de calor de similitudes...\n"," No hay suficientes palabras en el vocabulario para crear el mapa de calor\n","\n","============================================================\n","CONCLUSIONES DEL ANÁLISIS\n","============================================================\n","\n"," ESTADÍSTICAS DEL MODELO:\n","   • Tamaño del vocabulario: 6057 palabras\n","   • Dimensión de vectores: 50\n","   • Documentos procesados: 6812\n","\n"," COHERENCIA SEMÁNTICA:\n","   El modelo ha aprendido relaciones semánticas significativas:\n","\n"," PATRONES DESCUBIERTOS:\n","   1. Agrupación temática: Las palabras del mismo campo semántico\n","      tienden a estar cerca en el espacio vectorial.\n","   2. Contexto compartido: Palabras que aparecen en contextos\n","      similares tienen representaciones vectoriales cercanas.\n","   3. Relaciones sintácticas: El modelo captura relaciones\n","      gramaticales implícitas en los datos.\n","\n"," APLICACIONES POTENCIALES:\n","   • Búsqueda semántica en documentos\n","   • Sistemas de recomendación basados en contenido\n","   • Clasificación automática de textos\n","   • Detección de temas y clustering de documentos\n","   • Análisis de sentimientos contextualizado\n","\n"," LIMITACIONES OBSERVADAS:\n","   • Vocabulario limitado por el tamaño del corpus\n","   • Posible sesgo hacia términos más frecuentes\n","   • Necesidad de más datos para capturar relaciones complejas\n","\n","============================================================\n","\n","============================================================\n","EJEMPLO DE USO DEL MODELO\n","============================================================\n","\n","    # Para cargar y usar el modelo posteriormente:\n","    from gensim.models import Word2Vec\n","    \n","    # Cargar modelo\n","    modelo = Word2Vec.load('word2vec_model.model')\n","    \n","    # Buscar palabras similares\n","    similares = modelo.wv.most_similar('datos', topn=5)\n","    \n","    # Obtener vector de una palabra\n","    vector = modelo.wv['python']\n","    \n","    # Calcular similitud entre dos palabras\n","    similitud = modelo.wv.similarity('datos', 'información')\n","    \n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}